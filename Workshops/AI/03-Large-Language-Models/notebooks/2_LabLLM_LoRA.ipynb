{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVqac1pg809I"
      },
      "source": [
        "# LoRA Fine-Tuning Lab: T5-small Customer-Support Adapter\n",
        "\n",
        "Welcome!  \n",
        "This hands-on notebook will guide you, step by step, through fine-tuning a **T5-small** model with **LoRA (Low-Rank Adaptation)** so it can answer customer-support questions in a consistent, structured style.  \n",
        "Indeed, out-of-the-box **T5-small** has never seen our customer-support style or policy.  \n",
        "If you prompt it with “My order arrived late, I want a refund,” it answers vaguely, or not at all.  \n",
        "\n",
        "In this notebook we will:\n",
        "\n",
        "1. **Measure the baseline** and see how poorly vanilla T5 handles eight real support prompts.  \n",
        "2. **Attach a tiny LoRA adapter** (\\~ 9 MB, 8-rank) and fine-tune it on **just 250 examples**.  \n",
        "3. **Re-test the same prompts** to verify that the adapted model now produces concise, policy-compliant replies.  \n",
        "\n",
        "**Key takeaway:** with LoRA we upgrade a generic language model into a task-specialist in ~10 minutes on a free Colab GPU, without touching the original 60 M parameters.\n",
        "\n",
        "## What is LoRA (Low-Rank Adaptation)\n",
        "\n",
        "Instead of updating all hundreds of millions of parameters, LoRA freezes the original model and inserts two tiny matrices into selected linear layers (often the Q and V projections). Training adjusts only those low-rank “adapter” weights. So you need far less GPU memory, reach good quality with small datasets, and ship adapters (\\~10 MB) instead of full checkpoints (\\~2 GB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvH1ePiH809K"
      },
      "source": [
        "## 0 Environment Clean-up\n",
        "Before starting, you may want to delete any previous artefacts (checkpoints, logs, etc.) so the run is fresh and reproducible.  \n",
        "Feel free to skip this cell if you have nothing to clean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWEJ1_vBLlja"
      },
      "outputs": [],
      "source": [
        "# Remove previous training artefacts—run only if you need a fresh start\n",
        "!rm -rf t5-lora-out\n",
        "!rm -rf t5-small-lora-adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMkIRIWx809M"
      },
      "source": [
        "## 1 Install Dependencies\n",
        "We rely on the **Hugging Face Transformers** ecosystem plus two helper libraries:\n",
        "- **`transformers`**: model and trainer APIs.\n",
        "- **`datasets`**: efficient data loading from HF.\n",
        "- **`peft`** – adds LoRA and other parameter-efficient methods.  \n",
        "- **`accelerate`** – handles device placement (CPU / single-GPU / multi-GPU) transparently.\n",
        "\n",
        "Installation is one time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIvNNZPcg3T-"
      },
      "outputs": [],
      "source": [
        "# Transformers, Datasets, PEFT, and Accelerate (quiet install)\n",
        "!pip install -q transformers datasets peft accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ponHU0RFA8Ms"
      },
      "source": [
        "## 2 Baseline Check: How well does vanilla T5-small handle our task?\n",
        "\n",
        "Before we train anything, let's ask the out-of-the-box model to draft a refund reply.  \n",
        "Spoiler: its answer will be generic, overly long, or simply unrelated because T5-small has never been told what our support policy or tone should be.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjw3r_bMA631"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model_base = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "prompt = \"reply to this customer's email: My order arrived late and I want a refund.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Greedy decode to keep things deterministic and short\n",
        "outputs = model_base.generate(**inputs, max_new_tokens=120)\n",
        "print(\"Vanilla T5-small says:\\n\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkYjmNZ7809N"
      },
      "source": [
        "## 3 Load CSV Dataset and transform it to HF Dataset\n",
        "\n",
        "Upload to Colab filesystem the file of our dataset: `files/customer_support_lora_dataset_250`.  \n",
        "Our CSV file has two columns:\n",
        "\n",
        "| Column name   | What it contains                                 |\n",
        "|---------------|--------------------------------------------------|\n",
        "| `input_text`  | A raw customer request or complaint              |\n",
        "| `target_text` | The ideal structured reply we want the model to generate |\n",
        "\n",
        "We'll turn the CSV into a **HF Dataset** object so it plays nicely with the Trainer API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm3Hrq4o6bGd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Read the 250-row customer-support file\n",
        "df = pd.read_csv(\"customer_support_lora_dataset_250.csv\")\n",
        "ds = Dataset.from_pandas(df)\n",
        "\n",
        "print(\"Sample row:\") # quick sanity check\n",
        "ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HlULnbn809P"
      },
      "source": [
        "## 3 Tokenisation and Label Preparation\n",
        "\n",
        "Transformer models can't read raw text, they need **token IDs**.\n",
        "For sequence-to-sequence models like T5 we must prepare **two** sequences:\n",
        "\n",
        "1. **Source** - the customer request (`input_text`)  \n",
        "2. **Target** - the desired reply (`target_text`)  \n",
        "\n",
        "### Key details:\n",
        "\n",
        "- We call `tokenizer.as_target_tokenizer()` so the decoder uses its own special prefix tokens.  \n",
        "- We truncate to 128 tokens to keep batches small on modest GPUs.\n",
        "\n",
        "### What exactly are \"tokens\" (the 128-token limit)?  \n",
        "A token is not a word or a single character.\n",
        "Transformers work on sub-word units produced by a tokenizer (for T5 that's a SentencePiece model with a 32 k-item vocabulary). The rules are learned from large corpora and try to strike a balance:\n",
        "\n",
        "| Example text  | Tokens generated       | Notes                                 |\n",
        "| ------------- | ---------------------- | ------------------------------------- |\n",
        "| `tracking`    | `▁track`, `ing`        | the leading “▁” marks a word start    |\n",
        "| `refund`      | `▁refund`              | common words are often a single token |\n",
        "| `extra-large` | `▁extra`, `-`, `large` | punctuation becomes its own token     |\n",
        "\n",
        "Because tokens can be full words or fragments, the length in tokens is usually 1.3-1.6x shorter than counting raw characters but longer than counting full words.\n",
        "A 128-token limit therefore fits roughly 75-100 English words (fewer if the text contains many rare names, URLs, or emojis that split into multiple tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4z3IZjFrwMG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess(example):\n",
        "    # Encode source\n",
        "    model_inputs  = tokenizer(example[\"input_text\"],  max_length=128, truncation=True)\n",
        "    # Encode target as labels\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"target_text\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "ds_tok = ds.map(preprocess, remove_columns=ds.column_names)\n",
        "\n",
        "print(\"Input: \", ds[0][\"input_text\"])\n",
        "print(\"Input tokens: \", len(ds_tok[0][\"input_ids\"]))\n",
        "print(\"Target: \", ds[0][\"target_text\"])\n",
        "print(\"Target tokens: \", len(ds_tok[0][\"labels\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU43xk3P809R"
      },
      "source": [
        "## 4 Build Base Model and LoRA Configuration\n",
        "\n",
        "Here we load the vanilla **T5-small** (60 M parameters) and wrap it with a `peft.LoraConfig`.\n",
        "\n",
        "### Key hyper-parameters:\n",
        "\n",
        "| Parameter    | Role                                                          | Here |\n",
        "|--------------|---------------------------------------------------------------|------|\n",
        "| `r`          | Rank of the low-rank matrices (higher = more learning capacity)        | 8    |\n",
        "| `lora_alpha` | Scaling factor for the adapter’s update                       | 16   |\n",
        "| `target_modules` | Which weight matrices get adapters (we pick **q** & **v**) | [\"q\",\"v\"] |\n",
        "| `lora_dropout` | Regularisation inside adapters                               | 0.05 |\n",
        "\n",
        "### Why place LoRA adapters on q and v? What about the others?\n",
        "\n",
        "| Symbol | Full name                       | Role in self-attention         |\n",
        "| ------ | ------------------------------- | ------------------------------ |\n",
        "| **Q**  | **Query** projection            | asks “what am I looking for?”  |\n",
        "| **K**  | **Key** projection              | represents “what do I have?”   |\n",
        "| **V**  | **Value** projection            | holds the information to mix   |\n",
        "| **O**  | **Output** (final linear layer) | re-mixes heads after attention |\n",
        "\n",
        "A complete attention block has four projection matrices per head. Putting LoRA on all four gives maximum flexibility but also multiplies train-time memory.  \n",
        "Empirical sweet-spot: Research (LoRA paper §5 and several follow-ups) showed that adapting Q + V captures most task-specific gains while keeping parameter count and GPU RAM minimal. The intuition:\n",
        "- Queries (Q) change how each token attends to others.\n",
        "- Values (V) change what content is blended once attention scores are computed.\n",
        "\n",
        "Keys and the output layer matter too, but adjusting them yields diminishing returns for many language-generation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJwCKCFFrKgI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type      = TaskType.SEQ_2_SEQ_LM,  # generation task\n",
        "    r              = 8,      # rank of the LoRA matrices\n",
        "    lora_alpha     = 16,     # scaling\n",
        "    target_modules = [\"q\", \"v\"],  # project only query & value matrices\n",
        "    lora_dropout   = 0.05,\n",
        "    bias           = \"none\"\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(base_model, lora_cfg)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezlVpv2R809S"
      },
      "source": [
        "## 5 Training Arguments and Trainer Loop\n",
        "\n",
        "Hugging Face `Seq2SeqTrainer` takes care of the full training loop (forward, back-prop, gradient clipping, etc.).\n",
        "\n",
        "Important flags we set:\n",
        "\n",
        "- **`per_device_train_batch_size` = 16**, fits on a 12 GB GPU.  \n",
        "- **`num_train_epochs` = 30**, small dataset needs more passes.  \n",
        "- **`learning_rate` = 5e-4**, slightly higher than full-fine-tuning, because we’re optimising far fewer weights.  \n",
        "- **`save_strategy` = \"no\"**, skip checkpoints to save disk. You can change it to `\"epoch\"` if you want them.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_zAx2Eq2raxS"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir          = \"./t5-lora-out\",\n",
        "    per_device_train_batch_size = 16,\n",
        "    num_train_epochs    = 30,\n",
        "    learning_rate       = 5e-4,\n",
        "    logging_steps       = 5,\n",
        "    save_strategy       = \"no\",\n",
        "    report_to           = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model         = peft_model,\n",
        "    args          = training_args,\n",
        "    train_dataset = ds_tok,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD8UKp4g809T"
      },
      "source": [
        "## 6 Save Adapter and Tokenizer\n",
        "\n",
        "LoRA lets us store **only** the lightweight adapter, ~9 MB in this case. The base T5-small weights are **not duplicated**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbw5BG817UvV"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(\"t5-small-lora-adapter\")\n",
        "tokenizer.save_pretrained(\"t5-small-lora-adapter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8U36q-C809T"
      },
      "source": [
        "## 7 Load LoRA-Adapted Model for Inference\n",
        "\n",
        "We merge the adapter with the frozen base model at load time, then generate a reply for a sample complaint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG0hnG43qlAM"
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load base model\n",
        "cfg         = PeftConfig.from_pretrained(\"t5-small-lora-adapter\")\n",
        "base_model  = AutoModelForSeq2SeqLM.from_pretrained(cfg.base_model_name_or_path)\n",
        "# Load LoRA adapter\n",
        "model_lora  = PeftModel.from_pretrained(base_model, \"t5-small-lora-adapter\")\n",
        "\n",
        "# Test input\n",
        "prompt = \"generate reply: My order arrived late. I want a refund.\"\n",
        "# Tokenize the test input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "# Generate reply\n",
        "outputs = model_lora.generate(**inputs, max_new_tokens=80)\n",
        "\n",
        "print(\"LoRA reply:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI_MC1cU809U"
      },
      "source": [
        "## 8 Side-by-Side Evaluation (Base vs LoRA)\n",
        "\n",
        "Let’s run eight realistic prompts through both the vanilla T5-small and our LoRA-adapted version, then print the outputs in a table for quick eyeballing.  \n",
        "You should notice LoRA replies are:\n",
        "- More structured (e.g., include apology and next steps)  \n",
        "- Shorter and on brand  \n",
        "- Consistent JSON or bullet style, depending on your `target_text` examples  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0j9NQUt7e6F"
      },
      "outputs": [],
      "source": [
        "# Compare vanilla T5-small with LoRA fine‑tuned adapter on structured JSON output\n",
        "import json, pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Base (pre‑trained) model\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "# LoRA‑adapted model, make sure this path matches the one used in the training cell\n",
        "adapter_path = \"t5-small-lora-adapter\"\n",
        "cfg       = PeftConfig.from_pretrained(adapter_path)\n",
        "ft_model  = PeftModel.from_pretrained(\n",
        "    AutoModelForSeq2SeqLM.from_pretrained(cfg.base_model_name_or_path),\n",
        "    adapter_path\n",
        ")\n",
        "\n",
        "def generate(model, prompt):\n",
        "    ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    out = model.generate(**ids, max_new_tokens=120)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompts = [\n",
        "    \"Compose a response to this customer email: My order arrived late. I want a refund.\",\n",
        "    \"Draft a reply to this customer message: The product I received is damaged. What can I do?\",\n",
        "    \"Write a response to this email from a client: I received the wrong item in my order.\",\n",
        "    \"Create a reply for this customer's email: How can I return an item I purchased last week?\",\n",
        "    \"Formulate a response to the customer's email: I never received my order.\",\n",
        "    \"Respond to this message from the customer: Why was I charged twice for my order?\",\n",
        "    \"Prepare a reply to this client email: I need help tracking my shipment.\",\n",
        "    \"Construct a response for the customer's message: Can I exchange my item for a different size?\"\n",
        "]\n",
        "\n",
        "records = []\n",
        "for p in test_prompts:\n",
        "    records.append({\n",
        "        \"prompt\": p,\n",
        "        \"T5-base\": generate(base_model, p),\n",
        "        \"LoRA\":    generate(ft_model,  p)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(df.to_markdown(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csHeHzww809V"
      },
      "source": [
        "## 9  Next Steps\n",
        "\n",
        "1. **Quantisation**: combine LoRA with 8-bit weights using `bitsandbytes` library to shrink disk size and speed up inference.  \n",
        "2. **Hyper-parameter search**: try different ranks (`r`) and target modules (add **k** and **o** matrices) for possibly better accuracy.  \n",
        "3. **Objective metrics**: integrate BLEU, ROUGE-L, or a custom JSON validator to track quality over epochs.  \n",
        "4. **Deployment**: merge base + adapter and serve via FastAPI, Streamlit, or Hugging Face Inference Endpoints.  \n",
        "5. **Prompt scaffolding**: prepend `\"generate structured_reply:\"` automatically so end-users don’t need to remember it.  \n",
        "\n",
        "Happy fine-tuning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG70O8z6809V"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "12a8c86496d70dc5ec458316f0fdc66c09147f19c1386e8b96b40af59bd1e3d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
