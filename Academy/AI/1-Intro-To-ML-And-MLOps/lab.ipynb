{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Basic ML Model with Weather Dataset + MLflow Integration\n",
    "\n",
    "Welcome to this lab! Here you will learn how to:\n",
    "\n",
    "1. **Load and prepare a weather dataset**, with temperature and humidity data.\n",
    "2. **Train a Machine Learning model** using Scikit-learn, a powerful tool for Machine Learning in Python, to predict rain.\n",
    "3. **Evaluate the model** computing metrics to determine how well it makes predictions on new data.\n",
    "4. **Integrate MLflow**, one of the most used tool to track metrics, parameters, and model versions.\n",
    "\n",
    "We will follow a guided approach with detailed explanations at each step.  \n",
    "The first part focuses on Scikit-learn and the weather dataset. The second part extends the existing code with MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: From Data to Machine Learning Model (Supervised Learning with Scikit-learn)  \n",
    "\n",
    "### Objective  \n",
    "Build a **classification model** that can predict whether it will rain, using **temperature** and **humidity** as input data. The model will be trained using **Scikit-learn**, a powerful tool for Machine Learning in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the Dataset  \n",
    "\n",
    "Before training a Machine Learning model, it is essential to clean the data, as missing or incorrect values can compromise predictions. A well-prepared dataset allows the model to learn better and provide more accurate results.  \n",
    "\n",
    "For this lab, we will use an example dataset:  \n",
    "[Weather Test Data](https://raw.githubusercontent.com/boradpreet/Weather_dataset/refs/heads/master/Weather%20Test%20Data.csv)  \n",
    "\n",
    "The **Weather Test Data** dataset contains meteorological information collected at different times. Each row represents an observation with parameters such as **temperature**, **humidity**, **atmospheric pressure**, and other weather variables.  \n",
    "\n",
    "The goal of this dataset is to analyze weather patterns and use them to train a Machine Learning model capable of predicting future conditions, such as the probability of rain or temperature variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset URL \n",
    "url = \"https://raw.githubusercontent.com/boradpreet/Weather_dataset/refs/heads/master/Weather%20Test%20Data.csv\"\n",
    "\n",
    "# Load the dataset in a Pandas dataframe\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show first 5 rows\n",
    "df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration and Cleaning  \n",
    "\n",
    "To ensure our model works correctly, we first need to examine and prepare the dataset. Here are the key steps:  \n",
    "\n",
    "1. **Check for missing data**: Identify if there are any missing values, as they could compromise model training. If necessary, we can remove them or replace them with appropriate values.  \n",
    "2. **Convert the `Label` column**: Transform categorical values (*NoRain* and *Rain*) into numerical values (0 for *NoRain*, 1 for *Rain*), so the model can interpret them correctly.  \n",
    "3. **Select key features**: Choose only the most relevant columns (e.g., temperature and humidity) to simplify the model and improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# 2. Transform column 'RainToday' into numerical values\n",
    "df['RainToday'] = df['RainToday'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# 3. Feature selection \n",
    "features = ['MinTemp', 'MaxTemp', 'Humidity3pm', 'Humidity9am']\n",
    "\n",
    "X = df[features]\n",
    "y = df['RainToday']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting the Dataset into Training and Test Sets  \n",
    "\n",
    "To properly train and evaluate the model, we split the dataset into two parts:  \n",
    "\n",
    "- **X (features)**: Contains the information we will use for predictions, such as **temperature** and **humidity**.  \n",
    "- **y (target)**: Represents the variable we want to predict, i.e., whether it will rain (*Rain*) or not (*NoRain*).  \n",
    "\n",
    "We split the data into **80% training set** and **20% test set** for the following reasons:  \n",
    "\n",
    "1. **Model Training**  \n",
    "   - 80% of the data is used to teach the model to recognize patterns between features and the target variable.  \n",
    "\n",
    "2. **Model Evaluation**  \n",
    "   - The remaining 20% of the data is not used in training but serves to test the model on unseen data.  \n",
    "   - This helps us understand whether the model can make accurate predictions on new data.  \n",
    "\n",
    "3. **Avoid Overfitting**  \n",
    "   - If we tested the model on the same data it was trained on, we might get deceptively good results, as the model would have simply memorized them.  \n",
    "   - Using separate test data helps verify whether the model can generalize its predictions to real-world data.  \n",
    "\n",
    "This split is a crucial step in building a reliable model capable of making accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training dataset dimensions: {len(X_train)}\")\n",
    "print(f\"Test dataset dimensions: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building and Training the Model  \n",
    "\n",
    "Now that we have prepared the data, we can build and train a Machine Learning model. For this, we will use a classifier called [**RandomForestClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), one of the most widely used techniques for classification problems.  \n",
    "\n",
    "#### Why use **Random Forest**?  \n",
    "- It is a model based on **decision trees**, which divide the data into multiple steps to make accurate decisions.  \n",
    "- It is **robust** and works well with both numerical and categorical data.  \n",
    "- It is less sensitive to noisy data than a single decision tree because it combines multiple trees to improve accuracy.  \n",
    "\n",
    "The model will be trained using **temperature** and **humidity** data to predict whether there will be **rain** or not. After training, we will test it on new data to evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation  \n",
    "\n",
    "After training the model, we need to verify how accurate its predictions are. To do this, we will calculate **accuracy** and other evaluation metrics.  \n",
    "\n",
    "#### Why is model evaluation important?  \n",
    "A Machine Learning model is not useful if we do not know how reliable it is. Evaluation helps us understand:  \n",
    "- **Whether the model is learning correctly from the data** or simply memorizing answers (overfitting).  \n",
    "- **Whether it can be used on new data** and make realistic predictions.  \n",
    "\n",
    "#### Confusion Matrix  \n",
    "Besides accuracy, we will use the **confusion matrix**, a visual method that shows where the model makes correct predictions and where it makes mistakes.  \n",
    "- It helps identify **false positives** and **false negatives**, which are critical errors in many real-world scenarios.  \n",
    "- It is useful for improving the model, for example, by adjusting decision thresholds or balancing input data.  \n",
    "\n",
    "With these analyses, we can determine whether our model is ready for use or needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Prediction on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy and f1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}\")\n",
    "print(f\"F1-score of the model: {f1:.2f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a heatmap with Seaborn\n",
    "target_names = [\"No Rain\", \"Rain\"]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
    "\n",
    "# Add titles\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (Part 1)  \n",
    "\n",
    "In this first part, we followed a step-by-step process to build a Machine Learning model capable of predicting rain. Here’s what we did:  \n",
    "\n",
    "1. **Loaded the weather dataset** to analyze temperature, humidity, and other variables.  \n",
    "2. **Cleaned and prepared the data**, handling missing values and converting the target variable into a format the model can understand.  \n",
    "3. **Split the dataset** into training (80%) and test (20%) sets to properly train and evaluate the model.  \n",
    "4. **Built a classification model** using **RandomForestClassifier**, a powerful and robust algorithm.  \n",
    "5. **Evaluated the model’s performance** by calculating accuracy and analyzing the confusion matrix to identify errors.  \n",
    "\n",
    "Now that we have built the base model, in the next part, we will explore how to integrate **MLflow** to track experiments and further improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech: Installing and Configuring MLflow  \n",
    "\n",
    "### Objective  \n",
    "Set up a local **MLflow** instance to log experiments, monitor metrics, and manage Machine Learning models in an organized way.  \n",
    "\n",
    "### 1. Starting MLflow  \n",
    "\n",
    "To start MLflow locally, run the following command in the terminal:  \n",
    "\n",
    "```bash\n",
    "    mlflow ui\n",
    "```\n",
    "\n",
    "Once started, the graphical interface will be accessible at:  \n",
    "\n",
    "```\n",
    "    http://127.0.0.1:5000\n",
    "```\n",
    "\n",
    "This setup allows **saving experiments and models locally**, enabling tracking of different model versions, comparing evaluation metrics, and optimizing training processes.  \n",
    "\n",
    "In the next sections, we will see how to log parameters, metrics, and models directly within MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow in Production  \n",
    "\n",
    "In production environments and for clients, **MLflow is not run locally** but is integrated into a more robust and scalable infrastructure. This prevents issues related to manual experiment management and data persistence.  \n",
    "\n",
    "Common solutions include:  \n",
    "\n",
    "- **Docker Compose**  \n",
    "  - MLflow is started using a `docker-compose.yml` file, which configures a backend database and remote storage for saving experiments.  \n",
    "  - This approach is useful for controlled environments where a quick and reproducible setup is needed.  \n",
    "  - An example implementation is available in the internal repository:  \n",
    "    [kiratech/mlops-service-portfolio](https://github.com/kiratech/mlops-service-portfolio/tree/main).  \n",
    "\n",
    "- **Kubernetes (K8s)**  \n",
    "  - MLflow is deployed on a **Kubernetes cluster**, allowing scalable and centralized experiment management.  \n",
    "  - This approach is ideal for enterprise environments that require high levels of reliability, security, and scalability.  \n",
    "\n",
    "Both solutions rely on a **multi-container architecture**, which includes:  \n",
    "- **A persistent database** (e.g., PostgreSQL or MySQL) to store experiment metadata.  \n",
    "- **An S3 or MinIO storage** to save models and artifacts, ensuring secure and scalable data management.  \n",
    "\n",
    "These approaches ensure that MLflow can be reliably used in production, integrating with cloud or on-premise infrastructures for effective Machine Learning model management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MLflow Integration  \n",
    "\n",
    "Now, we will extend the existing code to **track our experiments** using **MLflow**. This will allow us to monitor the model training process, compare different configurations, and manage model versions in a structured way.  \n",
    "\n",
    "### Why integrate MLflow?  \n",
    "With MLflow, we can:  \n",
    "- **Log training parameters** (e.g., `n_estimators` for Random Forest) to compare different configurations.  \n",
    "- **Save evaluation metrics** (e.g., accuracy, F1-score) to monitor the model’s performance.  \n",
    "- **Store the trained model** to easily reload and reuse it in the future without retraining.  \n",
    "\n",
    "### Objective  \n",
    "Integrate MLflow into the existing code to **track and version models**, logging parameters, metrics, and artifacts in a structured way.  \n",
    "\n",
    "### 1. MLflow Configuration  \n",
    "Before we start tracking experiments, let's set up the necessary variables to use MLflow in this project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Set name of the experiment and tracking URI of local instance\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"weather_classification_experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that from our [web interface](http://127.0.0.1:5000), the new experiment is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logging Parameters, Metrics, and Model  \n",
    "\n",
    "With **MLflow**, we can automatically save and track various information during model training. This helps compare performances across different configurations and easily retrieve the best models.  \n",
    "\n",
    "Here’s what we can log:  \n",
    "\n",
    "- **Parameters** → Values used to configure the model, such as `n_estimators` (number of trees in Random Forest) and other hyperparameters.  \n",
    "- **Metrics** → Performance indicators of the model, such as **accuracy**, **F1-score**, precision, and recall.  \n",
    "- **Model** → The trained model version, which can be reloaded and reused without retraining.  \n",
    "\n",
    "By logging these elements, we can analyze and compare different model versions in a structured and reproducible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same type of model used in Part 1\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Execute 4 experiments to train multiple models \n",
    "n_estimators = [1, 10, 100, 500]\n",
    "\n",
    "for n_e in n_estimators:\n",
    "    # Create a new MLFlow run\n",
    "    with mlflow.start_run():\n",
    "        # Log Param\n",
    "        mlflow.log_param(\"n_estimators\", n_e)\n",
    "\n",
    "        # Create and train the model instance\n",
    "        rf_model = RandomForestClassifier(n_estimators=n_e, random_state=42)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Compute metrics\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1\", f1)\n",
    "\n",
    "        # Create heatmap with Seaborn\n",
    "        target_names = [\"No Rain\", \"Rain\"]\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
    "\n",
    "        # Add titles\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        # Save the plot as PNG\n",
    "        if not os.path.exists(\"dev/\"):\n",
    "            os.makedirs(\"dev/\")\n",
    "        plt.savefig(\"dev/confusion_matrix.png\")\n",
    "        plt.close()\n",
    "        # Save the confusion matrix on MLFlow as artifact\n",
    "        mlflow.log_artifact(\"dev/confusion_matrix.png\")\n",
    "\n",
    "        # Save the model on MLFlow\n",
    "        example_dict = {'MinTemp': 1.1, 'MaxTemp': 1.1, 'Humidity3pm': 1.1, 'Humidity9am': 1.1}\n",
    "        signature = infer_signature(model_input=example_dict)\n",
    "        mlflow.sklearn.log_model(rf_model, \"random_forest_model\", signature=signature)\n",
    "\n",
    "        print(f\"Experiment finished. Registered accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Viewing and Comparing Results  \n",
    "\n",
    "After logging parameters, metrics, and models, we can use **MLflow** to explore and compare different experiments.  \n",
    "\n",
    "MLflow provides a web interface accessible at:  \n",
    "\n",
    "```bash\n",
    "    http://127.0.0.1:5000\n",
    "```\n",
    "\n",
    "By accessing this interface, in the **Experiments** section, it will be possible to:  \n",
    "- **Examine the parameters** used in each experiment.  \n",
    "- **Compare metrics** across different model configurations.  \n",
    "- **View and download saved models**, making reuse and deployment easier.  \n",
    "\n",
    "This feature allows monitoring the model's performance evolution and quickly identifying the best configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading a Saved Model with MLflow  \n",
    "\n",
    "MLflow allows saving and reloading trained models easily, avoiding the need to retrain them every time.  \n",
    "\n",
    "To retrieve a saved model in MLflow, you need to copy the **run ID** of the executed experiment. This ID uniquely identifies each logged experiment and allows loading the corresponding model for future predictions.  \n",
    "\n",
    "This feature is particularly useful for:  \n",
    "- **Reusing a trained model** without repeating the training process.  \n",
    "- **Comparing different model versions** to choose the most effective one.  \n",
    "- **Integrating the model into applications or APIs**, without rebuilding it from scratch.  \n",
    "\n",
    "In the next sections, we will see how to perform this process in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Insert a real run_id you find in MLFlow UI\n",
    "RUN_ID = \"<run_id_of_your_experiment>\"\n",
    "\n",
    "loaded_model = mlflow.sklearn.load_model(f\"runs:/{RUN_ID}/random_forest_model\")\n",
    "\n",
    "# Verify accuracy\n",
    "y_loaded_pred = loaded_model.predict(X_test)\n",
    "acc_loaded = accuracy_score(y_test, y_loaded_pred)\n",
    "print(f\"Accuracy of the loaded model: {acc_loaded:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions  \n",
    "\n",
    "In this lab, we followed a complete process to build and monitor a Machine Learning model applied to weather data. Specifically, we:  \n",
    "\n",
    "1. **Created a classification model** using **Scikit-learn**, leveraging temperature and humidity to predict rain.  \n",
    "2. **Integrated MLflow** to track training parameters, log evaluation metrics, and manage model versions in a structured way.  \n",
    "3. **Explored and compared results** through the MLflow UI interface, reviewing different configurations and loading a saved model for future predictions.  \n",
    "\n",
    "This approach allows us to improve the Machine Learning model development process, making it more organized, reproducible, and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps  \n",
    "\n",
    "Now that we have built and tracked our model, we can explore further improvements and integrate the work into a more advanced workflow.  \n",
    "\n",
    "- **Hyperparameter Optimization**: Test different configurations of `n_estimators`, `max_depth`, and other model parameters, logging results in **MLflow** to identify the best combination.  \n",
    "- **Automation with CI/CD**: Integrate a **Continuous Integration/Continuous Deployment (CI/CD)** system to automatically train and deploy new model versions, reducing the risk of manual errors.  \n",
    "- **Model Monitoring in Production**: Implement a **model drift monitoring** system to detect any drops in accuracy over time and determine when retraining with new data is necessary.  \n",
    "\n",
    "These steps help transform the developed model into a robust and reliable system, ready for real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('lab_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbd6de99d3dee89886aa6f30475b6b26f6fe2ef531b8e041d1262739f0fd6851"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
