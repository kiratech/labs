{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_3UHSCaaZI"
      },
      "source": [
        "## What is Hugging Face and the Hugging Face Hub ?\n",
        "\n",
        "Hugging Face is an open-source-first AI company that curates Transformers, Diffusers, and other widely used libraries.\n",
        "The Hub (https://huggingface.co/) is a **GitHub for models & datasets**:\n",
        "- Free hosting for models, datasets, Spaces (demo apps).\n",
        "- Built-in versioning, README rendering, and model cards.\n",
        "- REST / Python SDK (huggingface_hub) for push-pull, inference endpoints, and gated access control.\n",
        "\n",
        "In practice you clone/push a repo exactly like Git, but via hf_hub_download() or model.push_to_hub()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hTQLYPZdUZp"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX-GYw-WbGrP"
      },
      "source": [
        "\n",
        "## What are providers on HF Inference Client and what is novita?\n",
        "\n",
        "The InferenceClient can route a request to different providers (back-ends):\n",
        "- huggingface (default): the standard Inference API hosted by HF.\n",
        "- novita (a third-party back-end): offers chat-optimized latency/pricing; API surface matches OpenAI-style /chat/completions.\n",
        "\n",
        "You switch simply by\n",
        "\n",
        "```python\n",
        "client = InferenceClient(provider=\"novita\", api_key=HF_TOKEN)\n",
        "```\n",
        "so the same client code can talk to multiple vendors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZG3akxmdqpT",
        "outputId": "456d745e-1ae4-49e7-8940-7a8e613e721f"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Inserisci il tuo token Hugging Face personale\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"ðŸ‘‰  Inserisci il tuo HF token: \")\n",
        "\n",
        "# Usa provider \"novita\" per il modello Mistral chat\n",
        "client = InferenceClient(provider=\"novita\", api_key=os.environ[\"HF_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D2_PSUlep8I"
      },
      "source": [
        "## Model spotlight - mistralai/Mistral-7B-Instruct-v0.3\n",
        "| -                  | Pros                                                                             | Cons                                                                        |\n",
        "| ------------------ | -------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| **Quality**        | Very strong chat & code for a 7 B model; outperforms LLaMA-7B/13B in many evals. | Still below GPT-4 / Claude-Opus tier on reasoning & long-context.           |\n",
        "| **Licence**        | Apache-2.0 â€“ commercial use allowed, no policy hoops.                            | Weight access sometimes gated; you need HF/token or local download.         |\n",
        "| **Speed / Memory** | Fits into 14 GB GPU (8-bit) â†’ cheap to host; responds \\~50-70 tok/s on T4.       | Context window 8 K; cannot process very long docs like GPT-4o-128K.         |\n",
        "| **LoRA**           | Adapter \\~120 MB; fine-tune on a Colab T4 in <30 min.                            | PEFT on Mistral needs to target correct linear layers (`q_proj`, `v_proj`). |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qogkSPAbbiD2"
      },
      "source": [
        "## Prompt Engineering - Zero-shot\n",
        "\n",
        "Ask the model to perform a task without examples: â€œTranslate the sentence to Italian: 'I love coffee'â€.\n",
        "\n",
        "Zero-shot works because large LMs have seen countless instructions during pre-training. It's the fastest way to test a capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd3cTO7LemPx",
        "outputId": "f971023b-1d60-4c6d-a6da-437b083c30c2"
      },
      "outputs": [],
      "source": [
        "prompt = \"A customer is requesting a refund because a product was delivered late. Write a professional reply.\"\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UF5mFW1cBNm"
      },
      "source": [
        "## Prompt Engineering - Few-shot\n",
        "\n",
        "Provide 1-3 exemples so the model infers pattern, tone, structure.\n",
        "\n",
        "Example:  \n",
        "Q1: Formal -> Casual  \n",
        "A1: Please contact me. -> Hit me up!  \n",
        "\n",
        "The next transformation aligns with the examples far better than zero-shot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeOQuFfRdrga",
        "outputId": "126939e8-9c72-48fd-a94b-00b5de91ab5f"
      },
      "outputs": [],
      "source": [
        "prompt = '''Example 1:\n",
        "Customer: I received my order late. I'd like a refund.\n",
        "Reply: We're sorry for the delay. Your refund will be processed within 3 business days.\n",
        "\n",
        "Example 2:\n",
        "Customer: My package arrived after the estimated delivery date. I want a refund.\n",
        "Reply: We apologize for the inconvenience. The refund has been approved and will be issued shortly.\n",
        "\n",
        "Now write a reply to:\n",
        "A customer is requesting a refund because a product was delivered late.\n",
        "'''\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExIj1lZyc9jE"
      },
      "source": [
        "## Chain-of-Thought Prompting\n",
        "\n",
        "Add â€œThink step by stepâ€ (or an explicit reasoning prompt).  \n",
        "The model reveals intermediate reasoning, often raising factual accuracy and allowing you to inspect/error-correct the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TgOb4owgnHn",
        "outputId": "7c8a24dc-f8f3-4de0-ac6c-fef324f80022"
      },
      "outputs": [],
      "source": [
        "prompt = \"A customer is requesting a refund because a product was delivered late. Think step by step and write a professional reply.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWUOoci-dQLx"
      },
      "source": [
        "## Role Prompting\n",
        "\n",
        "Pre-frame the assistant persona:\n",
        "\"You are an empathetic customer-support agentâ€¦\".  \n",
        "Sets vocabulary, tone, even policy constraints without touching the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5jnEUKwgoVM",
        "outputId": "10bc4383-505a-474c-80b9-af01e73c52be"
      },
      "outputs": [],
      "source": [
        "prompt = \"You are a customer support agent. A customer is asking for a refund due to a late delivery. Write a helpful and empathetic response.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8fryyF4dgJ2"
      },
      "source": [
        "## Prompt Patterns - Delimiters\n",
        "\n",
        "Wrap user context in clear markers (\"\"\" or ###), e.g.\n",
        "\n",
        "```txt\n",
        "\"\"\"CONTEXT\n",
        "long text â€¦\n",
        "\"\"\"\n",
        "Summarize the context in one bullet.\n",
        "```\n",
        "\n",
        "Prevents the model from mixing instructions with payload; boosts reliability in multi-part prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9doquBSg2jV",
        "outputId": "6d34e5a7-0075-40bd-d360-daca4b5b0a07"
      },
      "outputs": [],
      "source": [
        "prompt = '''Write a professional reply with the following structure:\n",
        "1. Greeting\n",
        "2. Apology for the delay\n",
        "3. Refund details\n",
        "4. Closing remark\n",
        "\n",
        "Customer: I received the product late and would like a refund.'''\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1ooDMboeG6k"
      },
      "source": [
        "## LoRA-style Fine-tuning (simulation vs real)\n",
        "\n",
        "Simulation: bake policy or domain hints inside the prompt ([POLICY] â€¦).  \n",
        "Real LoRA: train tiny adapter matrices (â‰ª1 % params) -> insert at Q/V/(K,O) projections. Needs only minutes & single GPU; adapter file <100 MB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koM2FhTJg2-q",
        "outputId": "26c6f6ba-4ee1-4976-b04a-ed7cdd23e6e8"
      },
      "outputs": [],
      "source": [
        "# Prompt standard\n",
        "prompt = \"A customer is requesting a refund because a product was delivered late. Write a professional reply.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(\"Standard Prompt:\\n\", completion.choices[0].message[\"content\"])\n",
        "\n",
        "# Prompt 'personalizzato' (simulazione LoRA con istruzioni nel prompt)\n",
        "prompt_custom_1 = '''[INTERNAL POLICY: Respond with empathy, offer refund without requiring customer to return the item, and apologize sincerely. Use concise and positive tone.]\n",
        "\n",
        "A customer is requesting a refund because a product was delivered late. Write a professional reply.'''\n",
        "\n",
        "completion_custom_1 = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_custom_1}],\n",
        ")\n",
        "print(\"Prompt personalizzato:\\n\", completion_custom_1.choices[0].message[\"content\"])\n",
        "\n",
        "# Prompt 'personalizzato' (simulazione LoRA con istruzioni nel prompt)\n",
        "prompt_custom_2 = '''[INTERNAL POLICY: Do not offer refunds, instead offer 15% discount code.]\n",
        "\n",
        "A customer is requesting a refund because a product was delivered late. Write a professional reply.'''\n",
        "\n",
        "completion_custom_2 = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_custom_2}],\n",
        ")\n",
        "print(\"Prompt personalizzato:\\n\", completion_custom_2.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6PVrBLGebEB"
      },
      "source": [
        "## Prompt for Structured Output (JSON)\n",
        "\n",
        "Explicitly instruct format + give a skeleton:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Return ONLY valid JSON:\n",
        "{\"greeting\":\"\", \"apology\":\"\", \"resolution\":\"\", \"action_type\":\"\", \"needs_follow_up\":false}\n",
        "```\n",
        "\n",
        "Model tends to obey if the request is precise and examples are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdBNug_3g3OX",
        "outputId": "bcb45f0e-6893-4fba-dada-6e160db982b3"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "You are a customer support assistant. A customer is requesting a refund because a product arrived late.\n",
        "\n",
        "Return the response in JSON format with the following fields:\n",
        "- greeting\n",
        "- apology\n",
        "- refund_policy\n",
        "- closing\n",
        "\n",
        "Example format:\n",
        "{\n",
        "  \"greeting\": \"...\",\n",
        "  \"apology\": \"...\",\n",
        "  \"refund_policy\": \"...\",\n",
        "  \"closing\": \"...\"\n",
        "}\n",
        "'''\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "12a8c86496d70dc5ec458316f0fdc66c09147f19c1386e8b96b40af59bd1e3d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
