{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt-Engineering Lab: Customer-Support Replies with **Mistral-7B-Instruct**\n",
        "\n",
        "In this lab you will experiment with six classic **prompt patterns**:\n",
        "\n",
        "1. **Zero-shot**: no examples, just the task.  \n",
        "2. **Few-shot**: show 1-2 examples first.  \n",
        "3. **Chain-of-Thought**: ask the model to reason step by step.  \n",
        "4. **Role / Persona**: tell the model \"You are a ...\".  \n",
        "5. **Structured output**: force JSON or bullet format.  \n",
        "6. **System / Policy instructions**: prepend internal guidelines.  \n",
        "\n",
        "By the end you’ll understand how tiny changes in wording can dramatically change an LLM’s answer.\n",
        "\n",
        "## What is Prompt-Engineering?\n",
        "Because large language models don’t have traditional \"APIs\", you steer them with natural-language prompts. Refining spacing, order, examples, and style (zero-shot, few-shot, chain-of-thought, role instructions, JSON schemas, etc.is called prompt engineering. Good prompts boost accuracy and consistency without touching the model’s weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_3UHSCaaZI"
      },
      "source": [
        "## 0  Install Hugging Face client library\n",
        "\n",
        "We only need **`huggingface_hub`** because we call the hosted Inference API. No heavyweight Transformers install is required.\n",
        "\n",
        "### What is Hugging Face and the Hugging Face Hub ?\n",
        "\n",
        "Hugging Face is an open-source-first AI company that curates Transformers, Diffusers, and other widely used libraries.\n",
        "The Hub (https://huggingface.co/) is a **GitHub for models & datasets**:\n",
        "- Free hosting for models, datasets, Spaces (demo apps).\n",
        "- Built-in versioning, README rendering, and model cards.\n",
        "- REST / Python SDK (huggingface_hub) for push-pull, inference endpoints, and gated access control.\n",
        "\n",
        "In practice you clone/push a repo exactly like Git, but via hf_hub_download() or model.push_to_hub()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hTQLYPZdUZp"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX-GYw-WbGrP"
      },
      "source": [
        "## 1  Authenticate and create an `InferenceClient`\n",
        "\n",
        "You need an **access token** to use the Mistral-7B endpoint hosted by provider **\"novita\"** on Hugging Face.\n",
        "\n",
        "### What are providers on HF Inference Client and what is novita?\n",
        "\n",
        "The InferenceClient can route a request to different providers (back-ends):\n",
        "- huggingface (default): the standard Inference API hosted by HF.\n",
        "- novita (a third-party back-end): offers chat-optimized latency/pricing. API surface matches OpenAI-style /chat/completions.\n",
        "\n",
        "You switch simply by\n",
        "\n",
        "```python\n",
        "client = InferenceClient(provider=\"novita\", api_key=HF_TOKEN)\n",
        "```\n",
        "so the same client code can talk to multiple vendors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZG3akxmdqpT",
        "outputId": "456d745e-1ae4-49e7-8940-7a8e613e721f"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Paste your personal HF token when prompted\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HF token: \")\n",
        "\n",
        "# Create the client (model is chosen later in each call)\n",
        "client = InferenceClient(provider=\"novita\", api_key=os.environ[\"HF_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D2_PSUlep8I"
      },
      "source": [
        "## Model spotlight - mistralai/Mistral-7B-Instruct-v0.3\n",
        "| -                  | Pros                                                                             | Cons                                                                        |\n",
        "| ------------------ | -------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| **Quality**        | Very strong chat and code for a 7 B model. Outperforms LLaMA-7B/13B in many evals. | Still below GPT-4 / Claude-Opus tier on reasoning and long-context.           |\n",
        "| **Licence**        | Apache-2.0, commercial use allowed.                            | Weight access sometimes gated. You need HF/token or local download.         |\n",
        "| **Speed / Memory** | Fits into 14 GB GPU (8-bit) so it is cheap to host. Responds \\~50-70 tok/s on T4.       | Context window 8 K. Cannot process very long docs like GPT-4o-128K.         |\n",
        "| **LoRA**           | Adapter \\~120 MB. Can be fine-tuned on a Colab T4 in <30 min.                            | PEFT on Mistral needs to target correct linear layers (`q_proj`, `v_proj`). |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qogkSPAbbiD2"
      },
      "source": [
        "## 2  Zero-shot prompt\n",
        "\n",
        "Ask the model to perform a task without examples: “Translate the sentence to Italian: 'I love coffee'”.\n",
        "\n",
        "Zero-shot works because large LMs have seen countless instructions during pre-training. It's the fastest way to test a capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd3cTO7LemPx",
        "outputId": "f971023b-1d60-4c6d-a6da-437b083c30c2"
      },
      "outputs": [],
      "source": [
        "prompt = \"A customer is requesting a refund because a product was delivered late. Write a professional reply.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UF5mFW1cBNm"
      },
      "source": [
        "## 3  Few-shot prompt\n",
        "\n",
        "Provide 1-3 examples so the model infers pattern, tone, structure.\n",
        "\n",
        "Example:  \n",
        "Q1: Formal -> Casual  \n",
        "A1: Please contact me. -> Hit me up!  \n",
        "\n",
        "The next transformation aligns with the examples far better than zero-shot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeOQuFfRdrga",
        "outputId": "126939e8-9c72-48fd-a94b-00b5de91ab5f"
      },
      "outputs": [],
      "source": [
        "prompt = '''Example 1:\n",
        "Customer: I received my order late. I'd like a refund.\n",
        "Reply: We're sorry for the delay. Your refund will be processed within 3 business days.\n",
        "\n",
        "Example 2:\n",
        "Customer: My package arrived after the estimated delivery date. I want a refund.\n",
        "Reply: We apologize for the inconvenience. The refund has been approved and will be issued shortly.\n",
        "\n",
        "Now write a reply to:\n",
        "A customer is requesting a refund because a product was delivered late.\n",
        "'''\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExIj1lZyc9jE"
      },
      "source": [
        "## 4  Chain-of-Thought prompt\n",
        "\n",
        "Add “Think step by step” (or an explicit reasoning prompt).  \n",
        "The model reveals intermediate reasoning, often raising factual accuracy and allowing you to inspect/error-correct the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TgOb4owgnHn",
        "outputId": "7c8a24dc-f8f3-4de0-ac6c-fef324f80022"
      },
      "outputs": [],
      "source": [
        "prompt = \"A customer is requesting a refund because a product was delivered late. Think step by step and write a professional reply.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWUOoci-dQLx"
      },
      "source": [
        "## 5  Role / Persona prompt\n",
        "\n",
        "Pre-frame the assistant persona:\n",
        "\"You are an empathetic customer-support agent…\".  \n",
        "Sets vocabulary, tone, even policy constraints without touching the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5jnEUKwgoVM",
        "outputId": "10bc4383-505a-474c-80b9-af01e73c52be"
      },
      "outputs": [],
      "source": [
        "prompt = \"You are a customer support agent. A customer is asking for a refund due to a late delivery. Write a helpful and empathetic response.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8fryyF4dgJ2"
      },
      "source": [
        "## 6 Structured Output\n",
        "\n",
        "Wrap user context in clear markers (\"\"\" or ###), e.g.\n",
        "\n",
        "```txt\n",
        "\"\"\"CONTEXT\n",
        "long text …\n",
        "\"\"\"\n",
        "Summarize the context in one bullet.\n",
        "```\n",
        "\n",
        "Prevents the model from mixing instructions with payload and boosts reliability in multi-part prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9doquBSg2jV",
        "outputId": "6d34e5a7-0075-40bd-d360-daca4b5b0a07"
      },
      "outputs": [],
      "source": [
        "prompt = '''Write a professional reply with the following structure:\n",
        "1. Greeting\n",
        "2. Apology for the delay\n",
        "3. Refund details\n",
        "4. Closing remark\n",
        "\n",
        "Customer: I received the product late and would like a refund.'''\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1ooDMboeG6k"
      },
      "source": [
        "## 7  System / Policy instructions (simulating LoRA-style policies)\n",
        "\n",
        "Simulation: bake policy or domain hints inside the prompt ([POLICY] ...). Notice how the reply changes when the policy changes.\n",
        "\n",
        "Real LoRA: train tiny adapter matrices (<<1 % params) and insert at Q/V/(K,O) projections of self-attention matrices. Needs only minutes, single GPU and the adapter file is <100 MB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koM2FhTJg2-q",
        "outputId": "26c6f6ba-4ee1-4976-b04a-ed7cdd23e6e8"
      },
      "outputs": [],
      "source": [
        "# Standard prompt\n",
        "prompt = \"A customer is requesting a refund because a product was delivered late. Write a professional reply.\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "\n",
        "print(\"Standard Prompt:\\n\", completion.choices[0].message[\"content\"])\n",
        "\n",
        "# Policy A: full refund, no return required\n",
        "prompt_custom_1 = '''[INTERNAL POLICY: Respond with empathy, offer refund without requiring customer to return the item, and apologize sincerely. Use concise and positive tone.]\n",
        "\n",
        "A customer is requesting a refund because a product was delivered late. Write a professional reply.'''\n",
        "\n",
        "completion_custom_1 = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_custom_1}],\n",
        ")\n",
        "print(\"Policy 1:\\n\", completion_custom_1.choices[0].message[\"content\"])\n",
        "\n",
        "# Policy B: no refund, offer discount code\n",
        "prompt_custom_2 = '''[INTERNAL POLICY: Do not offer refunds, instead offer 15% discount code.]\n",
        "\n",
        "A customer is requesting a refund because a product was delivered late. Write a professional reply.'''\n",
        "\n",
        "completion_custom_2 = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_custom_2}],\n",
        ")\n",
        "print(\"Policy 2:\\n\", completion_custom_2.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6PVrBLGebEB"
      },
      "source": [
        "## 8 Advanced: JSON-formatted reply\n",
        "\n",
        "Explicitly combining role, task, format and give a skeleton:\n",
        "\n",
        "```\n",
        "Return ONLY valid JSON:\n",
        "{\"greeting\":\"\", \"apology\":\"\", \"resolution\":\"\", \"action_type\":\"\", \"needs_follow_up\":false}\n",
        "```\n",
        "\n",
        "Model tends to obey if the request is precise and examples are consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdBNug_3g3OX",
        "outputId": "bcb45f0e-6893-4fba-dada-6e160db982b3"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "You are a customer support assistant. A customer is requesting a refund because a product arrived late.\n",
        "\n",
        "Return the response in JSON format with the following fields:\n",
        "- greeting\n",
        "- apology\n",
        "- refund_policy\n",
        "- closing\n",
        "\n",
        "Example format:\n",
        "{\n",
        "  \"greeting\": \"...\",\n",
        "  \"apology\": \"...\",\n",
        "  \"refund_policy\": \"...\",\n",
        "  \"closing\": \"...\"\n",
        "}\n",
        "'''\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        ")\n",
        "print(completion.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9  Take-aways\n",
        "\n",
        "- **Prompt engineering for rapid iteration**: you can prototype different behaviours in minutes, no fine-tuning needed.  \n",
        "- **System vs. User prompts**: internal policies (system) can override the tone and policy of the final answer.  \n",
        "- **Structured output**: JSON or numbered lists make post-processing trivial.  \n",
        "\n",
        "### Next experiments\n",
        "\n",
        "1. Add temperature/top-p arguments to explore creativity vs. safety of the model.  \n",
        "2. Test the same prompts on a different model (e.g., **Llama-3-8B-Instruct**) to compare style and latency.  \n",
        "3. Combine with *LoRA-style adapters* (see Notebook 2) for policy adherence without needing bulky prompts.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "12a8c86496d70dc5ec458316f0fdc66c09147f19c1386e8b96b40af59bd1e3d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
