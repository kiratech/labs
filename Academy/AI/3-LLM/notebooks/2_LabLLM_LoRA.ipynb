{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWEJ1_vBLlja"
      },
      "outputs": [],
      "source": [
        "!rm -rf t5-lora-out\n",
        "!rm -rf t5-small-lora-adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIvNNZPcg3T-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm3Hrq4o6bGd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "df = pd.read_csv(\"customer_support_lora_dataset_250.csv\")\n",
        "ds = Dataset.from_pandas(df)\n",
        "print(ds[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4z3IZjFrwMG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess(ex):\n",
        "    model_inputs  = tokenizer(ex[\"input_text\"],  max_length=128, truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(ex[\"target_text\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "ds_tok = ds.map(preprocess, remove_columns=ds.column_names)\n",
        "print(ds_tok[0].keys(), \"→ lengths:\", len(ds_tok[0][\"input_ids\"]), len(ds_tok[0][\"labels\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJwCKCFFrKgI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type      = TaskType.SEQ_2_SEQ_LM,\n",
        "    r              = 8,\n",
        "    lora_alpha     = 16,\n",
        "    target_modules = [\"q\", \"v\"],\n",
        "    lora_dropout   = 0.05,\n",
        "    bias           = \"none\"\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(base_model, lora_cfg)\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_zAx2Eq2raxS"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir          = \"./t5-lora-out\",\n",
        "    per_device_train_batch_size = 16,\n",
        "    num_train_epochs    = 30,\n",
        "    learning_rate       = 5e-4,\n",
        "    logging_steps       = 5,\n",
        "    save_strategy       = \"no\",\n",
        "    report_to           = \"none\",\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model         = peft_model,\n",
        "    args          = training_args,\n",
        "    train_dataset = ds_tok,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbw5BG817UvV"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(\"t5-small-lora-adapter\") \n",
        "tokenizer.save_pretrained(\"t5-small-lora-adapter\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG0hnG43qlAM"
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "cfg         = PeftConfig.from_pretrained(\"t5-small-lora-adapter\")\n",
        "base_model  = AutoModelForSeq2SeqLM.from_pretrained(cfg.base_model_name_or_path)\n",
        "model_lora  = PeftModel.from_pretrained(base_model, \"t5-small-lora-adapter\")\n",
        "\n",
        "prompt = \"generate reply: My order arrived late. I want a refund.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model_lora.generate(**inputs, max_new_tokens=80)\n",
        "\n",
        "print(\"→\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0j9NQUt7e6F"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compare vanilla T5-small with LoRA fine‑tuned adapter on structured JSON output\n",
        "import json, pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Base (pre‑trained) model\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "# LoRA‑adapted model – make sure this path matches the one used in the training cell\n",
        "adapter_path = \"t5-small-lora-adapter\"\n",
        "cfg       = PeftConfig.from_pretrained(adapter_path)\n",
        "ft_model  = PeftModel.from_pretrained(\n",
        "    AutoModelForSeq2SeqLM.from_pretrained(cfg.base_model_name_or_path),\n",
        "    adapter_path\n",
        ")\n",
        "\n",
        "def generate(model, prompt):\n",
        "    ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    out = model.generate(**ids, max_new_tokens=120)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompts = [\n",
        "    \"generate structured_reply: My order arrived late. I want a refund.\",\n",
        "    \"generate structured_reply: The product I received is damaged..\",\n",
        "    \"generate structured_reply: I correctly received my order. Thanks!\"\n",
        "]\n",
        "test_prompts = [\n",
        "    \"generate structured_reply: My order arrived late. I want a refund.\",\n",
        "    \"generate structured_reply: The product I received is damaged. What can I do?\",\n",
        "    \"generate structured_reply: I received the wrong item in my order.\",\n",
        "    \"generate structured_reply: How can I return an item I purchased last week?\",\n",
        "    \"generate structured_reply: I never received my order.\",\n",
        "    \"generate structured_reply: Why was I charged twice for my order?\",\n",
        "    \"generate structured_reply: I need help tracking my shipment.\",\n",
        "    \"generate structured_reply: Can I exchange my item for a different size?\"\n",
        "]\n",
        "\n",
        "records = []\n",
        "for p in test_prompts:\n",
        "    records.append({\n",
        "        \"prompt\": p,\n",
        "        \"T5-base\": generate(base_model, p),\n",
        "        \"LoRA\":    generate(ft_model,  p)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(df.to_markdown(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "12a8c86496d70dc5ec458316f0fdc66c09147f19c1386e8b96b40af59bd1e3d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
